Linear Regression
========================================================



```{r soluData}
library(AppliedPredictiveModeling)
data(solubility)
ls(pattern="solT")
dim(solTrainXtrans)
trainSol <- solTrainXtrans
trainSol$Solubility <- solTrainY
```
Some initial plots
```{r edaPlots}
library(lattice)

### Some initial plots of the data

xyplot(solTrainY ~ solTrainX$MolWeight, type = c("p", "g"),
       ylab = "Solubility (log)",
       main = "(a)",
       xlab = "Molecular Weight")
xyplot(solTrainY ~ solTrainX$NumRotBonds, type = c("p", "g"),
       ylab = "Solubility (log)",
       xlab = "Number of Rotatable Bonds")
bwplot(solTrainY ~ ifelse(solTrainX[,100] == 1, 
                          "structure present", 
                          "structure absent"),
       ylab = "Solubility (log)",
       main = "(b)",
       horizontal = FALSE)
```
Find the columns that are not fingerprints (i.e. the continuous
 predictors). grep will return a list of integers corresponding to
 column names that contain the pattern "FP".

```{r edaplots}
fingerprints <- grep("FP", names(solTrainXtrans))

library(caret)
featurePlot(solTrainXtrans[, -fingerprints],
            solTrainY,
            between = list(x = 1, y = 1),
            type = c("g", "p", "smooth"),
            labels = rep("", 2))

library(corrplot)

### We used the full namespace to call this function because the pls
### package (also used in this chapter) has a function with the same
### name.

corrplot::corrplot(cor(solTrainXtrans[, -fingerprints]), 
                   order = "hclust", 
                   tl.cex = .8)
```
Lets start modeling. We will save the various test results in a data frame
```{r linreg}
testResults <- data.frame(obs=solTestY)
lm.all <- lm(Solubility ~ ., data=trainSol)
summary(lm.all)
lmPred.all <- predict(lm.all, newdata=solTestXtrans)
testResults$lmPred.all <- lmPred.all
lmValues.all <- data.frame(obs=solTestY, pred=lmPred.all)
defaultSummary(lmValues.all)
```
Robust linear regression,
```{r robustreg}
library(MASS)
rlm.all <- rlm(Solubility ~ ., data=trainSol)
summary(rlm.all)
rlmPred.all <- predict(rlm.all, newdata=solTestXtrans)
testResults$rlmPred.all <- rlmPred.all
rlmValues.all <- data.frame(obs=solTestY, pred=rlmPred.all)
defaultSummary(rlmValues.all)
```
Doesn't do any better

Cross validation
```{r crossvalidate}
ctrl <- trainControl(method="cv", number=10)
set.seed(100)
lm.cv.all <- train(x=solTrainXtrans, y=solTrainY, method="lm", trControl=ctrl)
lm.cv.all
```
Plot results
```{r plots}
xyplot(solTrainY ~ predict(lm.cv.all), 
       type=c("p", "g"),
       xlab="Predicted",
       ylab="Observed")

xyplot(resid(lm.cv.all) ~ predict(lm.cv.all),
       type=c("p", "g"),
       xlab="Predicted",
       ylab="Residuals")
```
Make a smaller model by removing correlated variables
```{r remCor}
corThresh <- 0.75
tooHigh <- findCorrelation(cor(solTrainXtrans), corThresh)
corrPred <- names(solTrainXtrans)[tooHigh]
trainXfltrd <- solTrainXtrans[ , -tooHigh]
testXfltrd <- solTestXtrans[ , -tooHigh]
set.seed(100)
lm.cv.fltrd <- train(x=trainXfltrd, y=solTrainY, method="lm", trControl=ctrl)
lm.cv.fltrd
```
We can also use rlm, but that model does not tolerate singular matrices, so we will have to PCA the data 
first
```{r trainRlm}
set.seed(100)
lm.cv.rlm <- train(solTrainXtrans, solTrainY,
                   method="rlm",
                   preProcess="pca",
                   trControl=ctrl)
lm.cv.rlm
```

Lets compare the all and fltrd models over the test set
```{r compareAllFltrdRlm}
pred.cv.all <- predict(lm.cv.all, newdata=solTestXtrans)
pred.cv.fltrd <- predict(lm.cv.fltrd, newdata=testXfltrd)
pred.cv.rlm <- predict(lm.cv.rlm, newdata=solTestXtrans)
testResults$lmPred.cv.all <- pred.cv.all
testResults$lmPred.cv.fltrd <- pred.cv.fltrd
testResults$rlmPred.cv <- pred.cv.rlm
defaultSummary(data.frame(obs=solTestY, pred=pred.cv.all))
defaultSummary(data.frame(obs=solTestY, pred=pred.cv.fltrd))
defaultSummary(data.frame(obs=solTestY, pred=pred.cv.rlm))
```



##Partial Least Squares

```{r pls}
library(pls)
plsFit <- plsr(Solubility ~ ., data=trainSol)
perf.pls <- do.call("rbind", lapply(1:plsFit$ncomp, function(n){
                      pred.pls <- as.vector(predict(plsFit, newdata=solTestXtrans, ncomp=n))
                      defaultSummary(data.frame(obs=solTestY, pred=pred.pls))
                  }))
plot(perf.pls[,1], type="l", xlab="#components", ylab="RMSE")
lines(perf.pls[,2], type="l", col="red")
```
plsr can do K-fold or LOOCV via the validation argument, the PLS algorithm can be chosen using the method argument. Specialized plot function for visualizations.
train can also be used
```{r trainPLS}
set.seed(100)
plsTune <- train(solTrainXtrans, solTrainY, 
                 method="pls",
                 tuneLength=30,
                 trControl=ctrl,
                 preProc=c("center", "scale"))
pred.plsTune <- predict(plsTune, newdata=solTestXtrans)
defaultSummary(data.frame(obs=solTestY, pred=pred.plsTune))
```

## Section 6.3 Partial Least Squares

```{r plspcr}
set.seed(100)
plsTune <- train(x = solTrainXtrans, y = solTrainY,
                 method = "pls",
                 tuneGrid = expand.grid(ncomp = 1:20),
                 trControl = ctrl)
plsTune

testResults$plsTune <- predict(plsTune, solTestXtrans)

set.seed(100)
pcrTune <- train(x = solTrainXtrans, y = solTrainY,
                 method = "pcr",
                 tuneGrid = expand.grid(ncomp = 1:35),
                 trControl = ctrl)
pcrTune                  

plsResamples <- plsTune$results
plsResamples$Model <- "PLS"
pcrResamples <- pcrTune$results
pcrResamples$Model <- "PCR"
plsPlotData <- rbind(plsResamples, pcrResamples)

xyplot(RMSE ~ ncomp,
       data = plsPlotData,
       #aspect = 1,
       xlab = "# Components",
       ylab = "RMSE (Cross-Validation)",
       auto.key = list(columns = 2),
       groups = Model,
       type = c("o", "g"))

plsImp <- varImp(plsTune, scale = FALSE)
plot(plsImp, top = 25, scales = list(y = list(cex = .95)))
```

################################################################################
## Section 6.4 Penalized Models

 The text used the elasticnet to obtain a ridge regression model.
 There is now a simple ridge regression method.

```{r penalized}
ridgeGrid <- expand.grid(lambda = seq(0, .1, length = 15))

set.seed(100)
ridgeTune <- train(x = solTrainXtrans, y = solTrainY,
                   method = "ridge",
                   tuneGrid = ridgeGrid,
                   trControl = ctrl,
                   preProc = c("center", "scale"))
ridgeTune

print(update(plot(ridgeTune), xlab = "Penalty"))


enetGrid <- expand.grid(lambda = c(0, 0.01, .1), 
                        fraction = seq(.05, 1, length = 20))
set.seed(100)
enetTune <- train(x = solTrainXtrans, y = solTrainY,
                  method = "enet",
                  tuneGrid = enetGrid,
                  trControl = ctrl,
                  preProc = c("center", "scale"))
enetTune

plot(enetTune)

testResults$ridgeTune <- predict(ridgeTune, solTestXtrans)
testResults$enetune <- predict(enetTune, solTestXtrans)

################################################################################

Ridge-regression
```{r ridge}
library(elasticnet)
ridgeModel <- enet(x=as.matrix(solTrainXtrans), y=solTrainY, lambda=0.001)
```
